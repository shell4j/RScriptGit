corpus = Corpus(VectorSource(tolower(documents)))
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords("english"))
frequencies = DocumentTermMatrix(corpus)
word_frequencies = as.data.frame(as.matrix(frequencies))
words <- colnames(word_frequencies)
freq <- colSums(word_frequencies)
wordcloud(words, freq,
min.freq=sort(freq, decreasing=TRUE)[[400]],
colors=brewer.pal(8, "Dark2"),
random.color=TRUE)
}
makeWordCloud(emailsFromHillary[["EmailBody"]])
emailsToHillary <- dbGetQuery(db, "
SELECT p.Name Recipient,
ExtractedBodyText EmailBody
FROM Emails e
INNER JOIN EmailReceivers r ON r.EmailId=e.Id
INNER JOIN Persons p ON r.PersonId=P.Id
WHERE p.Name='Hillary Clinton'
AND e.ExtractedBodyText != ''
ORDER BY RANDOM()")
makeWordCloud(emailsToHillary[["EmailBody"]])
source('D:/RScriptGit/sentiment1.R', encoding = 'UTF-8')
source('D:/RScriptGit/sentiment1.R', encoding = 'UTF-8')
library(readr)
library(syuzhet)
library(RSQLite)
db <- dbConnect(dbDriver("SQLite"), "D:/Kaggle/HillaryEmail/output/database.sqlite")
#library(dplyr)
###选出hillary发送的文件内容
emailsFromHillary <- dbGetQuery(db, "
SELECT p.Name Sender,
ExtractedBodyText EmailBody
FROM Emails e
INNER JOIN Persons p ON e.SenderPersonId=P.Id
WHERE p.Name='Hillary Clinton'
AND e.ExtractedBodyText != ''
ORDER BY RANDOM()")
d<-get_nrc_sentiment(Emails$RawText)
td<-data.frame(t(d))
td_new <- data.frame(rowSums(td[2:7945]))
source('D:/RScriptGit/sentiment1.R', encoding = 'UTF-8')
source('D:/RScriptGit/sentiment1.R', encoding = 'UTF-8')
Emails <- data.frame(dbGetQuery(db,"SELECT * FROM Emails"))
###################################
#  Sentiment Analysis             #
###################################
d<-get_nrc_sentiment(Emails$RawText)
td<-data.frame(t(d))
td_new <- data.frame(rowSums(td[2:7945]))
#Transformation and  cleaning
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<-td_new[1:8,]
#Vizualisation
library("ggplot2")
qplot(sentiment, data=td_new2, weight=count, geom="histogram",fill=sentiment)+ggtitle("sentiment Email")
source('D:/RScriptGit/sentiment1.R', encoding = 'UTF-8')
summary(d)
class(d)
??get_nrc_sentiment
?syuzhet
??syuzhet
pander::pandoc.table(td_new2[, 1:8])
install.packages('pander')
pander::pandoc.table(td_new2[, 1:8])
pander::pandoc.table(td_new2[1:8,])
head(d)
source('D:/RScriptGit/sentiment1.R', encoding = 'UTF-8')
t(d)
??get_stanford_sentiment
sdf=get_stanford_sentiment(Emails$RawText)
library(syuzhet)
sdf=get_stanford_sentiment(Emails$RawText)
??get_percentage_values()
get_percentage_values(d)
t=get_percentage_values(d)
d
install.packages('lda')
install.packages('topicmodels')
library(topicmodels)
??"TopicModel-class"
??lda
source('D:/RScriptGit/topicModels.R')
source('D:/RScriptGit/topicModels.R')
num_topics = 15 # Max that memory limit can handle
vcorp = VCorpus(VectorSource(emailsFromHillary$ExtractedBodyText))
docterm_mat = DocumentTermMatrix(vcorp)
drop_inds = which(rowSums(as.matrix(docterm_mat))==0)
docterm_mat = docterm_mat[-drop_inds,]
num_topics = 15 # Max that memory limit can handle
fitted_lda = LDA(docterm_mat,k=num_topics)
summary(fitted_lda)
terms(fitted_lda,num_terms)
summary(emailsFromHillary)
vcorp = VCorpus(VectorSource(emailsFromHillary$EmailBody))
docterm_mat = DocumentTermMatrix(vcorp)
drop_inds = which(rowSums(as.matrix(docterm_mat))==0)
docterm_mat = docterm_mat[-drop_inds,]
num_topics = 15 # Max that memory limit can handle
fitted_lda = LDA(docterm_mat,k=num_topics)
terms(fitted_lda,num_terms)
source('D:/RScriptGit/sentiment1.R', encoding = 'UTF-8')
docterm_mat = DocumentTermMatrix(vcorp)
source('D:/RScriptGit/topicmodels1.R', encoding = 'UTF-8')
library(readr)
library(RSQLite)
library(topicmodels)
library(tm)
db <- dbConnect(dbDriver("SQLite"), "D:/Kaggle/HillaryEmail/output/database.sqlite")
emailsToHillary <- dbGetQuery(db, "
SELECT p.Name Recipient,
ExtractedBodyText EmailBody
FROM Emails e
INNER JOIN EmailReceivers r ON r.EmailId=e.Id
INNER JOIN Persons p ON r.PersonId=P.Id
WHERE p.Name='Hillary Clinton'
AND e.ExtractedBodyText != ''
ORDER BY RANDOM()")
vcorp = VCorpus(VectorSource(emailsToHillary$EmailBody))
docterm_mat = DocumentTermMatrix(vcorp)
drop_inds = which(rowSums(as.matrix(docterm_mat))==0)
docterm_mat = docterm_mat[-drop_inds,]
install.packages("drat", repos="https://cran.rstudio.com")
drat:::addRepo("dmlc")
install.packages("mxnet")
require(mxnet)
require(methods)
shape = c(1, 1)
lr = 0.01
x = mx.nd.ones(shape)
y = mx.nd.zeros(shape)
print(x)
n = 1000
tic = proc.time()
for (i in 1 : n) {
y = y + x *lr
}
toc = proc.time() - tic
as.array(y)
print(toc)
require(mxnet)
require(methods)
install.packages(Rcpp)
install.packages('Rcpp')
require(mxnet)
require(methods)
install.packages("mxnet")
caret()
library('syuzhet')
??syuzhet
library(topicmodels)
??"TopicModel-class"
library(readr)
library(syuzhet)
library(RSQLite)
db <- dbConnect(dbDriver("SQLite"), "D:/Kaggle/HillaryEmail/output/database.sqlite")
#library(dplyr)
###希拉里发送邮件的情感分析
Emails <- dbGetQuery(db, "
SELECT p.Name Sender,
ExtractedBodyText EmailBody
FROM Emails e
INNER JOIN Persons p ON e.SenderPersonId=P.Id
WHERE p.Name='Hillary Clinton'
AND e.ExtractedBodyText != ''
ORDER BY RANDOM()")
class(Emails)
head(Emails,2)
d<-get_nrc_sentiment(Emails$EmailBody)
td<-data.frame(t(d))
length(Emails)
td<-data.frame(t(d))
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<-td_new[1:8,]
td_new <- data.frame(rowSums(td[2:7945]))
#Transformation and  cleaning
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<-td_new[1:8,]
td
summary(td)
td_new <- data.frame(rowSums(td[2:1882]))
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<-td_new[1:8,]names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<-td_new[1:8,]
td_new <- data.frame(rowSums(td[2:1882]))
#Transformation and  cleaning
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<-td_new[1:8,]
library("ggplot2")
qplot(sentiment, data=td_new2, weight=count, geom="histogram",fill=sentiment)+ggtitle("Hillary's sending sentiment Email")
Emails <- dbGetQuery(db, "
SELECT p.Name Recipient,
ExtractedBodyText EmailBody
FROM Emails e
INNER JOIN EmailReceivers r ON r.EmailId=e.Id
INNER JOIN Persons p ON r.PersonId=P.Id
WHERE p.Name='Hillary Clinton'
AND e.ExtractedBodyText != ''
ORDER BY RANDOM()")
d<-get_nrc_sentiment(Emails$EmailBody)
#d1=get_sentiment(Emails$RawText)
td<-data.frame(t(d))
td_new <- data.frame(rowSums(td[2:4715]))
#Transformation and  cleaning
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<-td_new[1:8,]
library("ggplot2")
qplot(sentiment, data=td_new2, weight=count, geom="histogram",fill=sentiment)+ggtitle("Hillary's sending sentiment Email")
vcorp = VCorpus(VectorSource(emailsFromHillary$EmailBody))
docterm_mat = DocumentTermMatrix(vcorp)
drop_inds = which(rowSums(as.matrix(docterm_mat))==0)
docterm_mat = docterm_mat[-drop_inds,]
num_topics = 5 # Max that memory limit can handle
fitted_lda = LDA(docterm_mat,k=num_topics)
num_terms = 10
terms(fitted_lda,num_terms)
emailsFromHillary <- dbGetQuery(db, "
SELECT p.Name Sender,
ExtractedBodyText EmailBody
FROM Emails e
INNER JOIN Persons p ON e.SenderPersonId=P.Id
WHERE p.Name='Hillary Clinton'
AND e.ExtractedBodyText != ''
ORDER BY RANDOM()")
vcorp = VCorpus(VectorSource(emailsFromHillary$EmailBody))
docterm_mat = DocumentTermMatrix(vcorp)
drop_inds = which(rowSums(as.matrix(docterm_mat))==0)
docterm_mat = docterm_mat[-drop_inds,]
num_topics = 5 # Max that memory limit can handle
fitted_lda = LDA(docterm_mat,k=num_topics)
num_terms = 10
terms(fitted_lda,num_terms)
library(topicmodels)
library(tm)
db <- dbConnect(dbDriver("SQLite"), "D:/Kaggle/HillaryEmail/output/database.sqlite")
emailsFromHillary <- dbGetQuery(db, "
SELECT p.Name Sender,
ExtractedBodyText EmailBody
FROM Emails e
INNER JOIN Persons p ON e.SenderPersonId=P.Id
WHERE p.Name='Hillary Clinton'
AND e.ExtractedBodyText != ''
ORDER BY RANDOM()")
vcorp = VCorpus(VectorSource(emailsFromHillary$EmailBody))
docterm_mat = DocumentTermMatrix(vcorp)
drop_inds = which(rowSums(as.matrix(docterm_mat))==0)
docterm_mat = docterm_mat[-drop_inds,]
num_topics = 5 # Max that memory limit can handle
fitted_lda = LDA(docterm_mat,k=num_topics)
num_terms = 10
terms(fitted_lda,num_terms)
num_topics = 10 # Max that memory limit can handle
fitted_lda = LDA(docterm_mat,k=num_topics)
num_terms = 10
terms(fitted_lda,num_terms)
aa=terms(fitted_lda,num_terms)
write_csv("D:/Kaggle/HillaryEmail/topicOutput.csv",aa)
aa
aa=as.data.frame(aa)
write_csv("D:/Kaggle/HillaryEmail/topicOutput.csv",aa)
??write_csv
write_csv(aa,"D:/Kaggle/HillaryEmail/topicOutput.csv")
num_topics = 20 # Max that memory limit can handle
fitted_lda = LDA(docterm_mat,k=num_topics)
num_terms = 10
terms(fitted_lda,num_terms)
aa=terms(fitted_lda,num_terms)
write_csv(aa,"D:/Kaggle/HillaryEmail/topicOutput.csv")
aa=terms(fitted_lda,num_terms)
aa=as.data.frame(aa)
write_csv(aa,"D:/Kaggle/HillaryEmail/topicOutput.csv
)
/
\
""
")
aa=terms(fitted_lda,num_terms)
aa=as.data.frame(aa)
write_csv(aa,"D:/Kaggle/HillaryEmail/topicOutput.csv")
aa=terms(fitted_lda,num_terms)
aa=as.data.frame(aa)
write_csv(aa,"D:/Kaggle/HillaryEmail/topicOutput.csv")
num_topics = 20 # Max that memory limit can handle
fitted_lda = LDA(docterm_mat,k=num_topics)
num_terms = 10
terms(fitted_lda,num_terms)
aa=terms(fitted_lda,num_terms)
aa=as.data.frame(aa)
write_csv(aa,"D:/Kaggle/HillaryEmail/topicOutput.csv")
library('syuzhet')
??syuzhet
library(readr)
library(syuzhet)
library(RSQLite)
db <- dbConnect(dbDriver("SQLite"), "D:/Kaggle/HillaryEmail/output/database.sqlite")
#library(dplyr)
Emails <- data.frame(dbGetQuery(db,"SELECT * FROM Emails"))
d<-get_nrc_sentiment(Emails$RawText)
head(d)
sum(d)
afinn_vector <- get_sentiment(Emails$RawText, method="afinn")
head(afinn_vector)
afinn_vector
a=afinn_vector[]
summary(afinn_vector)
mode(afinn_vector)
a=afinn_vector[afinn_vector>0]
a
b=afinn_vector[afinn_vector<=0]
count(a)
sum(a)
sum(b)
??ggplot
as.data.frame(sum(a),sum(b))
aa=as.data.frame(sum(a),sum(b))
aa
aa=aa[row.names(c("negtive","postive"))]
aa
aa=cbind(row.names(c("negtive","postive")),aa)
aa=c(14164,41960)
aa=as.data.frame(aa,row.names = c("negtive","postive"))
aa
ggplot(aa)
library(ggplot2)
ggplot(aa)
aa=as.data.frame(aa,column.names = c("negtive","postive"))
aa
ggplot(aa)
barplot(aa)
barplot(c(14164,41960))
?barplot
barplot(c(14164,41960),col=c("negetive","positive"),)
barplot(c(14164,41960),names=c("negetive","positive"),col = c("lightblue", "mistyrose"))
library(readr)
library(RSQLite)
library(topicmodels)
library(tm)
db <- dbConnect(dbDriver("SQLite"), "D:/Kaggle/HillaryEmail/output/database.sqlite")
emailsToHillary <- dbGetQuery(db, "
SELECT p.Name Recipient,
ExtractedBodyText EmailBody
FROM Emails e
INNER JOIN EmailReceivers r ON r.EmailId=e.Id
INNER JOIN Persons p ON r.PersonId=P.Id
WHERE p.Name='Hillary Clinton'
AND e.ExtractedBodyText != ''
ORDER BY RANDOM()")
vcorp = VCorpus(VectorSource(emailsToHillary$EmailBody))
docterm_mat = DocumentTermMatrix(vcorp)
drop_inds = which(rowSums(as.matrix(docterm_mat))==0)
docterm_mat = docterm_mat[-drop_inds,]
num_topics = 25 # Max that memory limit can handle
fitted_lda = LDA(docterm_mat,k=num_topics)
summary(fitted_lda)
num_terms = 20
aa=terms(fitted_lda,num_terms)
aa
write_csv(aa,"D:/Kaggle/HillaryEmail/topicOutput.csv")
write_csv(as.data.frame(aa),"D:/Kaggle/HillaryEmail/topicOutput.csv")
write_csv(as.data.frame(aa),"D:/Kaggle/HillaryEmail/topicOutput.csv")
library(readr)
library(RSQLite)
library(topicmodels)
library(tm)
db <- dbConnect(dbDriver("SQLite"), "D:/Kaggle/HillaryEmail/output/database.sqlite")
emailsToHillary <- dbGetQuery(db, "
SELECT p.Name Recipient,
ExtractedBodyText EmailBody
FROM Emails e
INNER JOIN EmailReceivers r ON r.EmailId=e.Id
INNER JOIN Persons p ON r.PersonId=P.Id
WHERE p.Name='Hillary Clinton'
AND e.ExtractedBodyText != ''
ORDER BY RANDOM()")
vcorp = VCorpus(VectorSource(emailsToHillary$EmailBody))
docterm_mat = DocumentTermMatrix(vcorp)
drop_inds = which(rowSums(as.matrix(docterm_mat))==0)
docterm_mat = docterm_mat[-drop_inds,]
num_topics = 25 # Max that memory limit can handle
fitted_lda = LDA(docterm_mat,k=num_topics)
num_terms = 20
aa=terms(fitted_lda,num_terms)
write_csv(as.data.frame(aa),"D:/Kaggle/HillaryEmail/topicOutput1.csv")
install.packages('igraph')
install.packages('xgboost')
library(xgboost)
?xgboost
a=c(2,3)
b=c(4,5)
max(a,b)
for i in length(b)
for i in length(b){}
for i in length(b){ c=max(a[i],b[i])}
for i in 1:length(b){ c=max(a[i],b[i])}
for (i in 1:length(a)){c[i]=max(a[i],b[i])}
a[1]
length(a)
c=length(a)
for (i in 1:length(a)){c=max(a[i],b[i])}
c
c=rep(1:length(a))
c
for (i in 1:length(a)){c[i]=max(a[i],b[i])}
c
library("ggplot2", lib.loc="D:/software/R-3.2.2/library")
install.packages(c("foreign", "nlme"))
install.packages("devtools")
install.packages('tidyr')
library(tidyr)
??tidyr
data()
a=data("diamonds"
)
summary(a)
head(a,2)
a=data(diamonds)
a
a=diamonds
a
head(a)
a2=a %>% gather(cut,x,y,z)
a2=a %>% gather(cut,x,y,z) a
install.packages('dplyr')
library(dplyr)
?"dplyr"
library(data.table)
library(dplyr)
a2=a %>% gather(cut,x,y,z) a
iris %>%
group_by(Species) %>%
summarise(avg = mean(Sepal.Width)) %>%
arrange(avg)
a %>%
gather(cut,x,y,z)
library(RCPP)
install.packages('RCPP')
install.packages('Rcpp')
install.packages("Rcpp")
install.packages('RevoScaleR')
library(Rcpp)
?"Rcpp"
library(data.table)
?"data.table"
library(dplyr)
?"dplyr"
install.packages('sweave')
install.packages('Sweave')
install.packages('SWeave')
install.packages('pgfSweave')
install.packages('LyX')
\documentclass{article}
?Sweave
testfile <- system.file("Sweave", "Sweave-test-1.Rnw", package = "utils")
## enforce par(ask = FALSE)
options(device.ask.default = FALSE)
## create a LaTeX file
Sweave(testfile)
## This can be compiled to PDF by
## tools::texi2pdf("Sweave-test-1.tex")
## or outside R by
## Rcmd texify --pdf Sweave-test-1.tex
## if MiKTeX is available.
## create an R source file from the code chunks
Stangle(testfile)
## which can be sourced, e.g.
source("Sweave-test-1.R")
title: "xgboost算法参数初探"
title: "xgboost"
author: "YongHwang"
author: "Yong Hwang"
date: "January 16, 2016"
author: "YongHwang"
```{r, echo=TRUE}
plot(cars)
```
```{r,echo=FALSE}
```{r, echo=FALSE}
You can also embed plots,中文例子 for example:
sessionInfo()
plot(x, y, main='chinese中文', xlab="中文”)
@
??pdf.options
```{r,echo=FALSE}
library(knitr)
sessionInfo()
pdf.options(family='GB1')
install.packages('extrafont')
"env": { "LANG": "en_US.UTF-8"}
devtools::install_github("rstudio/rmarkdown")
install.packages('showtext')
devtools::install_github('rstudio/rticles')
cat("中文")
Sys.getlocale()
print('chinese')
Sys.getlocale()
###汉子的语法aaa汉子aaa
Sys.getlocale()
